{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the OLLAMA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\" : \"llama3.2\",\n",
    "    \"prompt\" : \"Hello, My name is Daniel Adnan\",\n",
    "    \"stream\" : False,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(actual_response)\n",
    "else: \n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding memory to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default OLLAMA does not preserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model\" : \"llama3.2\",\n",
    "    \"prompt\" : \"What is my name?\",\n",
    "    \"stream\" : False,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(actual_response)\n",
    "else: \n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ollama import chat as ollama_chat\n",
    "\n",
    "model = 'llama3.2'\n",
    "messages = []\n",
    "# Roles\n",
    "USER = 'user'\n",
    "ASSISTANT = 'assistant'\n",
    "\n",
    "def add_history(content, role):\n",
    "    messages.append({'role': role, 'content': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message):\n",
    "    add_history(message, USER)\n",
    "    response = ollama_chat(model=model, messages=messages, stream=False)\n",
    "    complete_message = ''\n",
    "    for line in response:\n",
    "        # Check if the line is a tuple and contains the 'message' key\n",
    "        if isinstance(line, tuple) and line[0] == 'message':\n",
    "            message_content = line[1].content\n",
    "            complete_message += message_content\n",
    "            # print(message_content, end='', flush=True)\n",
    "        # else:\n",
    "        #     print(\"Unexpected line format:\", line)\n",
    "    add_history(complete_message, ASSISTANT)\n",
    "    return complete_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = chat(\"Hello, my name is Shadab\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = chat(\"What is my name?\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "chat_response = chat(\"What is my name?\")\n",
    "print(chat_response)\n",
    "print(messages)\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets torch faiss-cpu matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pdf file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = \"random_story.pdf\"\n",
    "document = fitz.open(pdf_document)\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for page_num in range(len(document)):\n",
    "    page = document.load_page(page_num) \n",
    "    text = page.get_text()  \n",
    "    all_text += text \n",
    "\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text (splitting by paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into paragraphs (simple split by newline characters)\n",
    "def read_and_split_text(all_text):\n",
    "    \n",
    "    paragraphs = all_text.split('\\n')\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Split the text into paragraphs\n",
    "paragraphs = read_and_split_text(all_text)\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"sample: {i} paragraph: {paragraphs[i]} \\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = paragraphs[0]\n",
    "print (text)\n",
    "\n",
    "tokens_result=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "tokens_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding into vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=context_encoder(**tokens_result)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize and embed the input text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        outputs = context_encoder(**inputs)\n",
    "        embeddings.append(outputs.pooler_output)\n",
    "    return torch.cat(embeddings).detach().numpy()\n",
    "\n",
    "random.shuffle(paragraphs)\n",
    "\n",
    "context_embeddings = encode_contexts(paragraphs)\n",
    "\n",
    "# store the dimenstion of the vector embeddings\n",
    "paragraphs_column = context_embeddings.shape[1]\n",
    "print(paragraphs_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (with FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert list of numpy arrays into a single numpy array\n",
    "embedding_dim = paragraphs_column \n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index for the embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(context_embeddings_np)  # Add the context embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Encoder & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DPR question encoder and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and tokenizing sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who is Arif?'\n",
    "question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search context from input PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the index\n",
    "D, I = index.search(question_embedding, k=5)  # Retrieve top 5 relevant contexts\n",
    "print(\"D:\",D)\n",
    "print(\"I:\",I)\n",
    "\n",
    "print(\"Top 5 relevant contexts:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"{i+1}: {paragraphs[idx]}\")\n",
    "    print(f\"distance {D[0][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to search context from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=20): # return top 5 relevant contexts\n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "# Test the function\n",
    "question = \"What is the name of father of Arif?\"\n",
    "D, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate an answer using OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message):\n",
    "    add_history(message, USER)\n",
    "    response = ollama_chat(model=model, messages=messages, stream=False)\n",
    "    complete_message = ''\n",
    "    for line in response:\n",
    "        # Check if the line is a tuple and contains the 'message' key\n",
    "        if isinstance(line, tuple) and line[0] == 'message':\n",
    "            message_content = line[1].content\n",
    "            complete_message += message_content\n",
    "            # print(message_content, end='', flush=True)\n",
    "        # else:\n",
    "        #     print(\"Unexpected line format:\", line)\n",
    "    add_history(complete_message, ASSISTANT)\n",
    "    return complete_message\n",
    "\n",
    "def generate_answer_with_ollama(question, relevant_contexts):\n",
    "    context_text = \" \".join(relevant_contexts)\n",
    "    prompt = f\"Context: {context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = chat(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you summarize the story?\"\n",
    "D, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=20)\n",
    "\n",
    "relevant_contexts = [paragraphs[i] for i in I[0]]\n",
    "\n",
    "# print the relevant contexts\n",
    "for i, context in enumerate(relevant_contexts):\n",
    "    print(f\"{i+1}: {context}\\n\")\n",
    "\n",
    "answer = generate_answer_with_ollama(question, relevant_contexts)\n",
    "\n",
    "if answer:\n",
    "    print(answer)\n",
    "\n",
    "else:\n",
    "    print(\"No answer found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
