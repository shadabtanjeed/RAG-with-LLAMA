{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the OLLAMA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6nRequirement already satisfied: requests in ./.venv/lib64/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib64/python3.13/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib64/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib64/python3.13/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib64/python3.13/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Daniel! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\" : \"llama3.2\",\n",
    "    \"prompt\" : \"Hello, My name is Daniel Adnan\",\n",
    "    \"stream\" : False,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(actual_response)\n",
    "else: \n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding memory to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default OLLAMA does not preserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have that information. I'm a large language model, I don't have the ability to know your personal details or keep track of individual users. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name with me, I can certainly address you by that name if you'd like!\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\" : \"llama3.2\",\n",
    "    \"prompt\" : \"What is my name?\",\n",
    "    \"stream\" : False,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(actual_response)\n",
    "else: \n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6nRequirement already satisfied: ollama in ./.venv/lib64/python3.13/site-packages (0.4.4)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./.venv/lib64/python3.13/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./.venv/lib64/python3.13/site-packages (from ollama) (2.10.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib64/python3.13/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.13/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.13/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib64/python3.13/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib64/python3.13/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.venv/lib64/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib64/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ollama import chat as ollama_chat\n",
    "\n",
    "model = 'llama3.2'\n",
    "messages = []\n",
    "# Roles\n",
    "USER = 'user'\n",
    "ASSISTANT = 'assistant'\n",
    "\n",
    "def add_history(content, role):\n",
    "    messages.append({'role': role, 'content': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message):\n",
    "    add_history(message, USER)\n",
    "    response = ollama_chat(model=model, messages=messages, stream=False)\n",
    "    complete_message = ''\n",
    "    for line in response:\n",
    "        # Check if the line is a tuple and contains the 'message' key\n",
    "        if isinstance(line, tuple) and line[0] == 'message':\n",
    "            message_content = line[1].content\n",
    "            complete_message += message_content\n",
    "            # print(message_content, end='', flush=True)\n",
    "        # else:\n",
    "        #     print(\"Unexpected line format:\", line)\n",
    "    add_history(complete_message, ASSISTANT)\n",
    "    return complete_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Shadab! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat(\"Hello, my name is Shadab\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You told me your name earlier - it's Shadab! How can I assist you today, Shadab?\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat(\"What is my name?\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about you, including your name. This conversation just started, and I'm a large language model, I don't retain any personal data or information about individual users. Each time you interact with me, it's a new conversation, and I don't have any prior knowledge about you. Would you like to introduce yourself?\n",
      "[{'role': 'user', 'content': 'What is my name?'}, {'role': 'assistant', 'content': \"I don't have any information about you, including your name. This conversation just started, and I'm a large language model, I don't retain any personal data or information about individual users. Each time you interact with me, it's a new conversation, and I don't have any prior knowledge about you. Would you like to introduce yourself?\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "chat_response = chat(\"What is my name?\")\n",
    "print(chat_response)\n",
    "print(messages)\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6nRequirement already satisfied: transformers in ./.venv/lib64/python3.13/site-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib64/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib64/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib64/python3.13/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib64/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib64/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib64/python3.13/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib64/python3.13/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib64/python3.13/site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib64/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib64/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib64/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib64/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib64/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib64/python3.13/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib64/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib64/python3.13/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib64/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib64/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib64/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib64/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib64/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib64/python3.13/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib64/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib64/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib64/python3.13/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib64/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib64/python3.13/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib64/python3.13/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib64/python3.13/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib64/python3.13/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib64/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib64/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib64/python3.13/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib64/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib64/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib64/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib64/python3.13/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib64/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib64/python3.13/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib64/python3.13/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib64/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch faiss-cpu matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadab/Desktop/Github Repos/RAG with OLLAMA/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pdf file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6nRequirement already satisfied: PyMuPDF in ./.venv/lib64/python3.13/site-packages (1.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled on the banks of the mighty Padma River in Bangladesh, lived a boy named Arif. The village, called \n",
      "Balukandi, was a picturesque place where lush green rice paddies stretched endlessly, and the gentle hum of nature was a constant \n",
      "companion. Arif, a spirited twelve-year-old, was known for his curious mind and boundless energy.\n",
      " \n",
      "Arif’s family was not wealthy, but they were rich in love and traditions. His father, Rahim Mia, was a fisherman who spent long hours on the river, \n",
      "casting his net in hopes of a bountiful catch. His mother, Amina Begum, managed their small household and worked tirelessly in their vegetable \n",
      "garden. Despite their modest means, they ensured that Arif attended the local school, which was a short walk from their home.\n",
      " \n",
      "Every morning, after saying his prayers and helping his mother fetch water from the village well, Arif would grab his worn-out satchel and head \n",
      "to school. The path to school was one of his favorite parts of the day. It meandered through the village, past fields of golden mustard flowers, \n",
      "and under the shade of ancient banyan trees. Along the way, Arif would often stop to watch the village potter shaping clay or listen to the \n",
      "melodic call of the koel bird.\n",
      " \n",
      "At school, Arif’s favorite subject was science. His teacher, Mr. Kabir, often marveled at his knack for understanding complex ideas. Arif was \n",
      "particularly fascinated by the idea of electricity. He had heard stories of how, in the nearby town, homes lit up with bulbs that needed no oil \n",
      "lamps. The thought seemed magical to him, and he dreamed of one day bringing that magic to Balukandi.\n",
      " \n",
      "One evening, while sitting by the riverbank and helping his father mend fishing nets, Arif asked, “Baba, why don’t we have lights like the town \n",
      "does?”\n",
      " \n",
      "Rahim Mia chuckled and said, “It’s expensive, my boy. Besides, we’ve always lived with lanterns. Why change now?”\n",
      " \n",
      "But Arif wasn’t convinced. That night, he lay on his straw mattress, staring at the dim flicker of the oil lamp, and made up his mind. He would \n",
      "find a way to bring electricity to their village.\n",
      " \n",
      "Over the next few months, Arif started experimenting. He collected discarded wires and broken bulbs from the town’s scrap dealer, saving his \n",
      "school lunch money to buy small components. He spent hours reading his science textbook and pestering Mr. Kabir with questions. Slowly, \n",
      "piece by piece, he built a makeshift wind turbine using bamboo, an old bicycle dynamo, and plastic bottles.\n",
      " \n",
      "The day he tested his creation, half the village gathered to watch. With his friends spinning the turbine blades, the dynamo began to hum, and a \n",
      "tiny bulb flickered to life. The crowd erupted in cheers. Although it was a small step, it was proof that even a village boy could dream big.\n",
      " \n",
      "Encouraged by his success, Arif shared his idea with the village elders. He proposed building more turbines to power a few lights in the \n",
      "community center and the mosque. The elders, initially skeptical, eventually agreed. With their blessing and some donations, Arif expanded his \n",
      "project, involving other village children in the effort. They scavenged materials, built turbines, and even learned basic wiring.\n",
      " \n",
      "Months later, on a cool winter evening, the village of Balukandi experienced a moment of magic. The community center lit up with the glow of \n",
      "electric bulbs for the first time. Children danced with joy, and the elders offered prayers of gratitude. Arif’s parents, watching from the crowd, \n",
      "couldn’t hide their tears of pride.\n",
      " \n",
      "Arif’s story spread beyond Balukandi. Journalists from the town came to interview him, and he even received an invitation to a science \n",
      "exhibition in Dhaka. There, he showcased his wind turbine project and spoke about the importance of renewable energy for rural communities. \n",
      "His simple yet impactful idea won hearts and awards.\n",
      " \n",
      "Years later, Arif’s dream of lighting up Balukandi grew into a larger mission. With the help of government grants and NGOs, he brought solar \n",
      "panels and microgrids to not just his village but several others in the region. He became an engineer, but he always remained the same humble \n",
      "boy who once wandered through mustard fields, dreaming of a brighter future for his people.\n",
      " \n",
      "Balukandi became a symbol of hope, and Arif’s journey inspired countless others to believe in the power of dreams and determination. And \n",
      "every time the village lit up at dusk, the people of Balukandi remembered the boy who turned a spark of curiosity into a beacon of light.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  \n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = \"random_story.pdf\"\n",
    "document = fitz.open(pdf_document)\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for page_num in range(len(document)):\n",
    "    page = document.load_page(page_num) \n",
    "    text = page.get_text()  \n",
    "    all_text += text \n",
    "\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text (splitting by paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0 paragraph: Once upon a time, in a small village nestled on the banks of the mighty Padma River in Bangladesh, lived a boy named Arif. The village, called \n",
      "\n",
      "sample: 1 paragraph: Balukandi, was a picturesque place where lush green rice paddies stretched endlessly, and the gentle hum of nature was a constant \n",
      "\n",
      "sample: 2 paragraph: companion. Arif, a spirited twelve-year-old, was known for his curious mind and boundless energy. \n",
      "\n",
      "sample: 3 paragraph: Arif’s family was not wealthy, but they were rich in love and traditions. His father, Rahim Mia, was a fisherman who spent long hours on the river, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the text into paragraphs (simple split by newline characters)\n",
    "def read_and_split_text(all_text):\n",
    "    \n",
    "    paragraphs = all_text.split('\\n')\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Split the text into paragraphs\n",
    "paragraphs = read_and_split_text(all_text)\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"sample: {i} paragraph: {paragraphs[i]} \\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPRContextEncoderTokenizer(name_or_path='facebook/dpr-ctx_encoder-single-nq-base', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled on the banks of the mighty Padma River in Bangladesh, lived a boy named Arif. The village, called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2320,  2588,  1037,  2051,  1010,  1999,  1037,  2235,  2352,\n",
       "         22704,  2006,  1996,  5085,  1997,  1996, 10478, 23731,  2314,  1999,\n",
       "          7269,  1010,  2973,  1037,  2879,  2315, 10488,  2546,  1012,  1996,\n",
       "          2352,  1010,  2170,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = paragraphs[0]\n",
    "print (text)\n",
    "\n",
    "tokens_result=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "tokens_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding into vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRContextEncoderOutput(pooler_output=tensor([[ 4.0093e-01,  4.2511e-02, -2.9443e-01,  9.9187e-02,  3.7222e-01,\n",
       "          5.6648e-01,  2.2446e-01, -1.0303e-01,  8.5561e-02, -7.3476e-01,\n",
       "         -1.7684e-01, -3.4557e-01, -3.9370e-01,  6.4019e-01,  2.8712e-01,\n",
       "          1.1167e-01,  3.9522e-01,  2.3242e-01, -3.3190e-01, -2.9161e-01,\n",
       "         -7.0793e-01,  5.1561e-02,  3.4618e-01,  3.1245e-01,  7.7873e-01,\n",
       "         -6.1256e-02,  2.3302e-01, -6.3971e-02, -1.4170e-02,  1.0972e-01,\n",
       "          9.8490e-02,  3.3719e-01,  2.3089e-01, -5.8903e-01, -7.6483e-01,\n",
       "         -2.7139e-01, -1.3550e-01,  1.7743e-01,  4.9149e-02, -7.8392e-01,\n",
       "         -2.0239e-01, -4.1871e-01,  4.0637e-01,  6.4429e-02, -3.0704e-01,\n",
       "         -7.6216e-01, -9.4178e-01,  6.6036e-01, -3.7666e-01, -1.3964e-01,\n",
       "          3.0464e-01,  5.4273e-01,  1.9491e-01, -5.3098e-01,  2.5661e-01,\n",
       "          4.5500e-01, -4.4324e-01,  5.0109e-02,  1.2105e-01, -5.4375e-01,\n",
       "          1.3316e+00,  8.7864e-01,  1.0550e+00,  4.1776e-01, -8.7826e-02,\n",
       "          5.2390e-01,  4.6784e-01, -4.7481e-01,  3.5440e-01,  4.6639e-01,\n",
       "         -2.5911e-01,  1.5619e-01, -6.8952e-01,  4.0331e-01, -2.1711e-01,\n",
       "          3.7258e-02,  5.0432e-01,  1.4126e-01, -1.9108e-01,  4.1600e-01,\n",
       "         -2.8707e-01, -9.5239e-02,  8.1897e-01,  1.8207e-01,  2.4052e-01,\n",
       "         -4.4737e-01, -3.8157e-01,  1.8387e-01, -1.0354e-02, -3.1181e-01,\n",
       "         -8.0291e-01, -4.1264e-01,  4.4984e-02,  9.5030e-01,  5.9351e-01,\n",
       "         -1.2228e-02,  2.0634e-01, -3.3835e-01, -1.0075e-02,  6.7800e-02,\n",
       "          2.3438e-01,  3.8401e-01, -2.3984e-01, -4.2119e-01, -2.2142e-01,\n",
       "          5.4054e-01,  6.0151e-01,  7.7772e-02, -3.2012e-01, -1.7520e-01,\n",
       "         -4.2391e-01,  4.9490e-01,  2.5958e-01, -3.2657e-01,  1.3921e-01,\n",
       "          2.4487e-01,  4.6399e-01,  1.4147e-01, -7.5668e-02, -1.8577e-01,\n",
       "         -6.9883e-01,  2.4466e-01,  1.3303e-01,  3.1792e-01,  1.7113e-01,\n",
       "         -4.3454e-01, -3.9691e-01, -5.2027e-01, -8.0884e-01,  3.4067e-02,\n",
       "          2.2719e-01,  1.1348e-01, -1.0119e-01, -1.0491e-01, -9.7401e-01,\n",
       "         -5.0681e-02, -1.2240e-01,  3.5091e-03, -4.1123e-01,  1.8570e-01,\n",
       "         -6.3179e-01,  3.1795e-02, -6.1835e-01, -2.1216e-01,  3.5236e-01,\n",
       "         -2.6573e-01,  4.5295e-01,  1.7046e-02,  6.1336e-02, -2.0474e-01,\n",
       "         -3.5801e-03,  3.3230e-02, -9.3310e-02, -4.5925e-01, -1.0901e-01,\n",
       "         -5.4087e-02, -4.7853e-01, -8.4316e-02,  2.8325e-01,  3.0077e-02,\n",
       "         -8.3335e-02,  7.1620e-01,  1.5560e-01,  4.6980e-02,  2.6300e-01,\n",
       "         -3.1861e-01, -9.0714e-02, -1.5191e-01, -6.1886e-01,  6.8077e-02,\n",
       "          6.1142e-02,  3.1967e-01, -7.5355e-02, -2.6745e-01, -6.4539e-01,\n",
       "          6.8242e-01,  3.1003e-01,  3.1101e-01, -2.7597e-01, -3.5747e-01,\n",
       "         -8.2020e-01, -3.1425e-01,  1.0982e-01,  2.4863e-01, -1.0070e-01,\n",
       "          1.8281e-01,  3.6076e-02,  3.5690e-01,  9.1336e-02,  5.2327e-01,\n",
       "         -6.0313e-01, -5.1844e-01, -1.8627e-01, -6.3983e-01,  8.0516e-01,\n",
       "         -2.8664e-01, -4.6405e-01,  9.8168e-01, -4.7850e-02,  3.7365e-01,\n",
       "          1.9212e-01, -2.1799e-01,  3.4246e-01, -2.2722e-01, -5.3371e-01,\n",
       "          3.7177e-01, -4.2833e-01,  7.3642e-01,  7.4028e-01, -5.0451e-01,\n",
       "          5.2725e-01,  6.2942e-01,  2.1113e-01,  1.4826e-01,  3.1086e-01,\n",
       "         -3.4337e-01,  7.2171e-02,  7.3722e-03,  1.4048e-02, -2.0836e-01,\n",
       "          5.4024e-01, -6.8839e-01,  5.0144e-02,  9.8657e-02,  4.4299e-01,\n",
       "          2.4981e-01, -1.1181e-02, -4.7412e-01,  3.4617e-01, -8.4614e-01,\n",
       "         -8.7599e-01, -1.8107e-01,  8.3592e-03, -1.4379e-01,  1.9757e-01,\n",
       "         -2.0542e-01,  5.2350e-01, -4.4269e-01,  1.9153e-01,  3.7671e-01,\n",
       "          1.6956e-01,  6.0697e-01, -5.2882e-01,  3.4763e-02,  3.0884e-02,\n",
       "         -2.7715e-02, -2.2655e-01,  6.7846e-01, -3.2034e-02,  8.9525e-02,\n",
       "         -3.8021e-01,  2.5686e-01,  6.6470e-01,  1.5694e-01,  3.8210e-01,\n",
       "          1.3333e-02, -3.5462e-01,  2.0210e-01, -1.8023e-02, -3.5991e-02,\n",
       "         -5.5893e-02,  3.7875e-01, -2.1867e-01,  1.0648e-02, -9.8521e-02,\n",
       "         -9.1310e-01, -3.2867e-01, -4.9366e-01,  8.3168e-01,  3.8739e-01,\n",
       "          1.0075e+00,  6.9134e-02, -2.1537e-01,  2.0125e-01,  6.4348e-02,\n",
       "         -1.0642e-01,  4.7557e-01, -1.7770e-01,  2.3402e-01, -4.5523e-01,\n",
       "          5.9615e-01,  2.3359e-01,  2.6705e-03, -1.5157e-01,  6.1063e-01,\n",
       "         -5.1178e-01, -2.1410e-01,  1.7320e-01,  1.1112e-01, -7.2825e-02,\n",
       "          3.4165e-01,  6.1695e-01, -4.1989e-01,  4.3936e-03,  2.9762e-01,\n",
       "          4.9596e-01, -1.2778e-02,  5.1421e-01,  9.8793e-02, -1.7848e-01,\n",
       "         -3.4485e-01,  3.6009e-01, -7.0944e-01,  3.7841e-01,  8.7366e-01,\n",
       "          1.4680e-01, -1.9910e-01, -1.4539e-01, -5.7743e+00,  4.4310e-01,\n",
       "         -4.4362e-02,  4.3102e-02, -2.8919e-01,  1.1770e-01,  2.8019e-01,\n",
       "          1.2027e-01,  1.1836e-01,  1.8581e-01, -8.4763e-01,  7.9116e-02,\n",
       "          1.5685e-02, -3.1837e-01, -2.4691e-01,  5.0055e-01,  1.1674e-01,\n",
       "          3.8069e-01, -6.6379e-01,  4.4497e-02, -6.0383e-01, -3.9504e-01,\n",
       "         -8.2688e-01,  8.3358e-01,  4.4102e-01,  1.6097e-02, -6.3496e-01,\n",
       "         -1.7954e-01, -8.7913e-01,  1.3943e-01, -1.6429e-01, -3.3430e-01,\n",
       "         -1.9252e-01, -9.7099e-02, -2.2700e-01, -1.7019e-01,  3.3736e-02,\n",
       "         -5.7497e-01,  1.9298e-01, -3.9874e-01, -6.9963e-01, -2.9894e-01,\n",
       "         -3.5015e-01, -3.3393e-01, -1.3347e-01, -2.8566e-01, -2.4101e-01,\n",
       "          2.8070e-02,  1.3693e-01,  2.8056e-01, -2.4152e-02, -4.7166e-01,\n",
       "          2.1494e-01, -5.4964e-01,  2.2048e-01,  1.2651e-01,  2.6254e-01,\n",
       "         -1.9260e-01, -2.6877e-01, -3.9738e-02,  2.3503e-01, -1.1636e-01,\n",
       "         -1.9861e-04, -3.1873e-01,  4.5621e-01, -5.4610e-01, -1.8290e-02,\n",
       "         -2.4990e-01, -2.3219e-01,  7.3374e-01, -6.1854e-01,  3.4880e-01,\n",
       "         -1.2549e-01, -7.5972e-01, -1.0675e-01,  5.6129e-03, -9.9050e-02,\n",
       "          4.5394e-02,  2.5581e-01,  8.0328e-03, -4.1749e-01, -1.4806e-01,\n",
       "          2.7028e-01,  2.4793e-01, -2.2211e-02, -9.0525e-03, -1.6864e-02,\n",
       "         -2.5013e-01, -1.5852e-01,  5.1996e-01,  1.1482e+00, -2.0197e-01,\n",
       "         -3.0577e-01,  8.5584e-02,  5.9142e-01, -4.2596e-02, -2.7834e-01,\n",
       "          1.4937e-01,  5.7955e-01, -7.5474e-02,  3.7136e-01,  1.8049e-01,\n",
       "         -1.2051e-01,  1.9277e-01,  2.8388e-01,  3.7684e-01, -3.2505e-01,\n",
       "         -2.4858e-01,  1.2807e+00,  3.3601e-01, -2.3364e-01,  3.8039e-01,\n",
       "         -3.3436e-01,  2.3670e-01,  8.5687e-01, -5.9593e-01,  4.4914e-01,\n",
       "          4.5369e-01,  1.6117e-01, -1.0740e-01,  2.0912e-01, -2.0096e-01,\n",
       "         -2.8185e-01, -3.5692e-01, -1.3557e-01, -5.6921e-01,  1.9736e-01,\n",
       "         -4.2381e-02, -3.9094e-03, -3.6735e-01,  4.3152e-01, -2.3279e-01,\n",
       "         -7.5774e-01, -2.8069e-01, -3.8923e-01, -1.3042e-01, -4.8481e-01,\n",
       "         -1.4047e+00,  3.8922e-01,  4.4722e-01, -5.8145e-01, -2.1186e-02,\n",
       "          1.1702e-01, -1.2923e-01,  4.4561e-01, -7.2932e-02, -3.2456e-02,\n",
       "          1.1860e-01, -7.1589e-02, -2.8332e-01,  3.9543e-01, -1.2584e-01,\n",
       "          4.8130e-01,  2.8027e-01, -1.1404e+00, -2.5122e-01,  1.5546e-01,\n",
       "         -1.2430e-01, -2.3774e-01,  8.6369e-02,  3.7744e-01, -2.4764e-01,\n",
       "         -6.9202e-01,  2.1722e-01,  1.2173e-01,  5.0012e-01,  2.4336e-01,\n",
       "         -5.3400e-02,  5.3743e-01,  2.0556e-01,  1.8962e-01, -3.9663e-01,\n",
       "         -3.4234e-02, -2.6895e-01,  1.1363e-01,  6.5837e-02, -5.7453e-01,\n",
       "         -4.5809e-01,  5.0820e-01, -2.6449e-01,  2.2855e-02,  8.4254e-02,\n",
       "         -2.4462e-01,  2.4926e-02,  6.6542e-01, -2.2166e-01, -2.9067e-01,\n",
       "         -5.2080e-01, -1.0341e+00,  1.5977e-01, -4.2758e-01,  3.5699e-02,\n",
       "          3.8762e-01,  4.8399e-01, -2.4475e-01, -7.8265e-02, -4.7238e-01,\n",
       "          8.8143e-02,  9.2546e-02,  8.6749e-02, -5.0038e-01, -3.8489e-01,\n",
       "         -2.3480e-01, -9.4757e-02,  3.0378e-01, -4.2324e-01,  1.1752e-01,\n",
       "         -4.6240e-02,  4.5838e-02,  3.1896e-01, -4.1585e-01, -7.3361e-01,\n",
       "          2.9466e-02,  2.7870e-02,  1.8173e-01, -7.9837e-02,  2.3784e-01,\n",
       "         -7.6724e-02,  1.9295e-02, -3.9850e-01,  5.4921e-01, -7.6728e-02,\n",
       "          5.8810e-01, -2.2836e-01, -1.3216e-01, -3.0275e-01,  8.3044e-01,\n",
       "         -4.3470e-01, -2.3469e-01, -9.1121e-02,  1.3289e-01,  3.8488e-01,\n",
       "          2.2256e-01,  3.2719e-01, -6.5531e-01, -5.7519e-01,  1.4683e-01,\n",
       "         -5.7372e-01, -5.9059e-02,  5.3536e-01,  2.0144e-02, -1.0325e-02,\n",
       "          1.6274e-01, -4.2980e-01,  5.5597e-01, -2.0274e-01, -4.3151e-02,\n",
       "          2.6966e-01, -3.6311e-01, -1.4507e-02,  2.6743e-01, -5.9666e-01,\n",
       "         -3.1704e-01, -5.8253e-02,  1.5327e-01, -6.0822e-01,  1.8026e-01,\n",
       "          1.5284e-02, -8.5288e-02, -4.4879e-03,  1.2010e-01, -7.0681e-02,\n",
       "         -7.5849e-01,  2.1788e-01, -1.2229e-01,  3.1730e-02, -3.2813e-01,\n",
       "         -1.6066e-01,  1.2962e-01, -7.9726e-01,  8.9074e-02, -1.4977e-01,\n",
       "          8.1513e-01,  1.3763e-01,  2.8596e-01, -1.7850e-01,  3.3375e-01,\n",
       "          4.7967e-01,  4.0059e-02, -3.9014e-01, -3.5895e-01, -3.8612e-02,\n",
       "          1.5497e-02, -4.3781e-01,  9.5362e-02, -4.6784e-01,  5.4275e-01,\n",
       "         -3.1384e-01, -1.3858e-01,  1.2097e-01, -3.7867e-01,  4.6005e-01,\n",
       "          5.0245e-01,  2.3997e-02,  1.4544e-01, -2.0226e-01,  2.2505e-01,\n",
       "         -1.0022e-01,  1.9067e-01,  2.1411e-01, -6.8657e-01,  3.5962e-01,\n",
       "          7.7183e-01,  1.2745e-01,  2.8230e-02,  1.2736e-01,  7.9933e-01,\n",
       "          3.5574e-01, -3.6809e-01,  3.5516e-01,  3.9338e-01, -2.5902e-01,\n",
       "         -2.7060e-01,  1.6667e-01, -1.0412e+00, -6.3362e-01,  5.7906e-02,\n",
       "         -3.1027e-01,  5.6032e-01, -1.0235e-01,  5.2196e-02, -1.1087e-01,\n",
       "          8.1252e-01, -6.9750e-02,  2.0193e-01,  4.3911e-01,  3.1067e-01,\n",
       "         -1.7081e-01, -5.7528e-01,  4.8922e-01, -2.5347e-01,  3.0868e-01,\n",
       "         -4.1603e-01, -1.6233e-01,  8.6034e-01,  1.6689e-02, -2.5287e-01,\n",
       "         -4.2388e-01, -2.5383e-02,  6.2242e-01,  2.3442e-01, -1.2457e-01,\n",
       "          8.1592e-02,  6.9134e-01,  3.8955e-01,  2.4511e-01,  3.6281e-01,\n",
       "          8.4261e-01, -4.4748e-03,  2.5347e-02,  3.3421e-01,  4.1278e-01,\n",
       "          4.5437e-01, -2.4680e-01,  5.9218e-01,  3.6284e-01,  3.9094e-01,\n",
       "          3.3873e-01, -7.5486e-01,  3.6288e-01,  4.3951e-01, -2.3883e-01,\n",
       "          2.7166e-01, -2.7364e-01,  4.4909e-02,  3.9086e-01,  7.5672e-02,\n",
       "         -7.6789e-01, -4.9498e-01, -1.8093e-01,  7.5403e-01,  3.2814e-01,\n",
       "         -5.9011e-01,  4.3867e-01,  1.0314e-02, -7.3795e-03,  2.5162e-01,\n",
       "         -4.4155e-01,  5.1636e-01,  7.7318e-02,  2.2907e-01, -5.3203e-01,\n",
       "          4.3333e-02, -2.0943e-01,  2.9672e-01, -8.3947e-02,  2.0180e-01,\n",
       "          1.2286e-01, -4.1011e-04, -3.6182e-02, -4.0617e-01, -6.4862e-01,\n",
       "         -7.2137e-02,  1.8185e-01,  1.2640e-01, -2.6952e-01, -7.4678e-01,\n",
       "          2.7630e-01,  5.0748e-01,  2.1393e-01,  2.0412e-01, -7.7713e-02,\n",
       "         -1.4868e-01,  6.5474e-03, -5.8171e-02,  1.6893e-01, -4.0838e-01,\n",
       "         -6.7279e-01, -6.3558e-01,  5.0784e-01,  7.0043e-01,  1.6548e-01,\n",
       "          2.9520e-02,  5.4990e-02, -5.5714e-01, -4.1555e-01,  6.8966e-01,\n",
       "         -6.3147e-02, -7.7806e-02, -1.0840e+00, -1.9862e-01,  4.5892e-01,\n",
       "         -6.7997e-01,  2.8254e-01, -9.4772e-01, -1.5472e-01, -3.0809e-01,\n",
       "          5.7246e-01, -6.3635e-01,  5.5329e-01,  2.2960e-02, -1.0552e+00,\n",
       "         -2.3147e-01,  4.8495e-01, -6.7910e-03, -6.5668e-02,  6.0868e-01,\n",
       "          2.6413e-01,  1.0064e-01, -2.3212e-01, -2.1305e-02, -1.1789e-01,\n",
       "          7.7997e-01, -2.1213e-01, -9.1894e-02,  1.7417e-01, -4.9543e-01,\n",
       "          7.2934e-01, -3.5418e-01,  6.3136e-01,  4.9227e-01, -3.4045e-01,\n",
       "          2.9120e-01,  4.2081e-02,  5.4125e-02,  2.5841e-01, -9.4001e-01,\n",
       "         -4.0551e-01,  1.8313e-01, -2.9799e-01]], grad_fn=<SliceBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs=context_encoder(**tokens_result)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize and embed the input text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "def encode_contexts(text_list):\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        outputs = context_encoder(**inputs)\n",
    "        embeddings.append(outputs.pooler_output)\n",
    "    return torch.cat(embeddings).detach().numpy()\n",
    "\n",
    "random.shuffle(paragraphs)\n",
    "\n",
    "context_embeddings = encode_contexts(paragraphs)\n",
    "\n",
    "# store the dimenstion of the vector embeddings\n",
    "paragraphs_column = context_embeddings.shape[1]\n",
    "print(paragraphs_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (with FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert list of numpy arrays into a single numpy array\n",
    "embedding_dim = paragraphs_column \n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index for the embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(context_embeddings_np)  # Add the context embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Encoder & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DPR question encoder and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and tokenizing sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who is Arif?'\n",
    "question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search context from input PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[ 80.44499  86.9055   89.54919  93.60104 101.62082]]\n",
      "I: [[18  5 21  4 24]]\n",
      "Top 5 relevant contexts:\n",
      "1: Arif’s family was not wealthy, but they were rich in love and traditions. His father, Rahim Mia, was a fisherman who spent long hours on the river,\n",
      "distance 80.44499206542969\n",
      "\n",
      "2: Arif’s story spread beyond Balukandi. Journalists from the town came to interview him, and he even received an invitation to a science\n",
      "distance 86.90550231933594\n",
      "\n",
      "3: Balukandi became a symbol of hope, and Arif’s journey inspired countless others to believe in the power of dreams and determination. And\n",
      "distance 89.54918670654297\n",
      "\n",
      "4: Rahim Mia chuckled and said, “It’s expensive, my boy. Besides, we’ve always lived with lanterns. Why change now?”\n",
      "distance 93.60104370117188\n",
      "\n",
      "5: But Arif wasn’t convinced. That night, he lay on his straw mattress, staring at the dim flicker of the oil lamp, and made up his mind. He would\n",
      "distance 101.62081909179688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search the index\n",
    "D, I = index.search(question_embedding, k=5)  # Retrieve top 5 relevant contexts\n",
    "print(\"D:\",D)\n",
    "print(\"I:\",I)\n",
    "\n",
    "print(\"Top 5 relevant contexts:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"{i+1}: {paragraphs[idx]}\")\n",
    "    print(f\"distance {D[0][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to search context from question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[ 73.19446   83.350555  95.97923   96.402756 101.032196]]\n",
      "Indices: [[18 21 24  5  4]]\n"
     ]
    }
   ],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=20): # return top 5 relevant contexts\n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "# Test the function\n",
    "question = \"What is the name of father of Arif?\"\n",
    "D, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate an answer using OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message):\n",
    "    add_history(message, USER)\n",
    "    response = ollama_chat(model=model, messages=messages, stream=False)\n",
    "    complete_message = ''\n",
    "    for line in response:\n",
    "        # Check if the line is a tuple and contains the 'message' key\n",
    "        if isinstance(line, tuple) and line[0] == 'message':\n",
    "            message_content = line[1].content\n",
    "            complete_message += message_content\n",
    "            # print(message_content, end='', flush=True)\n",
    "        # else:\n",
    "        #     print(\"Unexpected line format:\", line)\n",
    "    add_history(complete_message, ASSISTANT)\n",
    "    return complete_message\n",
    "\n",
    "def generate_answer_with_ollama(question, relevant_contexts):\n",
    "    context_text = \" \".join(relevant_contexts)\n",
    "    prompt = f\"Context: {context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = chat(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Rahim Mia chuckled and said, “It’s expensive, my boy. Besides, we’ve always lived with lanterns. Why change now?”\n",
      "\n",
      "2: project, involving other village children in the effort. They scavenged materials, built turbines, and even learned basic wiring.\n",
      "\n",
      "3: The day he tested his creation, half the village gathered to watch. With his friends spinning the turbine blades, the dynamo began to hum, and a\n",
      "\n",
      "4: couldn’t hide their tears of pride.\n",
      "\n",
      "5: Months later, on a cool winter evening, the village of Balukandi experienced a moment of magic. The community center lit up with the glow of\n",
      "\n",
      "6: Balukandi became a symbol of hope, and Arif’s journey inspired countless others to believe in the power of dreams and determination. And\n",
      "\n",
      "7: tiny bulb flickered to life. The crowd erupted in cheers. Although it was a small step, it was proof that even a village boy could dream big.\n",
      "\n",
      "8: community center and the mosque. The elders, initially skeptical, eventually agreed. With their blessing and some donations, Arif expanded his\n",
      "\n",
      "9: Arif’s story spread beyond Balukandi. Journalists from the town came to interview him, and he even received an invitation to a science\n",
      "\n",
      "10: garden. Despite their modest means, they ensured that Arif attended the local school, which was a short walk from their home.\n",
      "\n",
      "11: piece by piece, he built a makeshift wind turbine using bamboo, an old bicycle dynamo, and plastic bottles.\n",
      "\n",
      "12: and under the shade of ancient banyan trees. Along the way, Arif would often stop to watch the village potter shaping clay or listen to the\n",
      "\n",
      "13: Years later, Arif’s dream of lighting up Balukandi grew into a larger mission. With the help of government grants and NGOs, he brought solar\n",
      "\n",
      "14: But Arif wasn’t convinced. That night, he lay on his straw mattress, staring at the dim flicker of the oil lamp, and made up his mind. He would\n",
      "\n",
      "15: companion. Arif, a spirited twelve-year-old, was known for his curious mind and boundless energy.\n",
      "\n",
      "16: electric bulbs for the first time. Children danced with joy, and the elders offered prayers of gratitude. Arif’s parents, watching from the crowd,\n",
      "\n",
      "17: His simple yet impactful idea won hearts and awards.\n",
      "\n",
      "18: casting his net in hopes of a bountiful catch. His mother, Amina Begum, managed their small household and worked tirelessly in their vegetable\n",
      "\n",
      "19: panels and microgrids to not just his village but several others in the region. He became an engineer, but he always remained the same humble\n",
      "\n",
      "20: Encouraged by his success, Arif shared his idea with the village elders. He proposed building more turbines to power a few lights in the\n",
      "\n",
      "The story is about a 12-year-old boy named Arif from the village of Balukandi. Arif comes up with an innovative idea to use bamboo, an old bicycle dynamo, and plastic bottles to build a wind turbine, which initially meets skepticism but eventually wins over the villagers.\n",
      "\n",
      "With the help of his friends and family, Arif successfully builds and tests his turbine, bringing light to the village for the first time. The community is inspired by his determination and ingenuity, and soon, Balukandi becomes a symbol of hope and progress.\n",
      "\n",
      "As Arif grows older, he expands his efforts, building solar panels and microgrids to power electricity in several villages, becoming an engineer and making a lasting impact on his community. Throughout the story, Arif's passion for innovation and determination inspire others, demonstrating that even small ideas can have a significant impact when pursued with courage and perseverance.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you summarize the story?\"\n",
    "D, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=20)\n",
    "\n",
    "relevant_contexts = [paragraphs[i] for i in I[0]]\n",
    "\n",
    "# print the relevant contexts\n",
    "for i, context in enumerate(relevant_contexts):\n",
    "    print(f\"{i+1}: {context}\\n\")\n",
    "\n",
    "answer = generate_answer_with_ollama(question, relevant_contexts)\n",
    "\n",
    "if answer:\n",
    "    print(answer)\n",
    "\n",
    "else:\n",
    "    print(\"No answer found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
